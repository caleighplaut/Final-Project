---
title: "Final Project"
author: "Meredith Manley and Caleigh Plaut"
date: "11/17/17"
output: 
  html_document:
    fig_height: 3
    fig_width: 5
  pdf_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---

```{r, setup, include=FALSE}
require(mosaic)   # Load additional packages here 

# Some customization.  You can alter or delete as desired (if you know what you are doing).
trellis.par.set(theme=theme.mosaic()) # change default color scheme for lattice
knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```



# Data Collection
```{r}
# Load the necessary packages
library(tidyverse)
library(httr)
library(yelpr)
library(glmnet)
library(MASS)
library(ggplot2)
library(rpart)
library(ggmap)

# Code provided by Alex to obtain data from Yelp API
devtools::install_github("jennybc/ryelp")
res <- POST("https://api.yelp.com/oauth2/token",
            body = list(grant_type = "client_credentials",
                        client_id = "t5Vxmm0MO0Cb6FV6y7RfOg",
                        client_secret = "CnukNBSKQMF1CNNk9BGX8H1ZCj4JwYdRnZmiaZkBbelBHD8Q9lZhZTaGuyAahZqS"))
token <- content(res)$access_token

# Looking at restaurants in the top 10 largest cities in the US
yelp <- "https://api.yelp.com"
term <- "restaurant"
location <- c("New York City, NY", "Los Angeles, CA", "Chicago, IL", "Houston,TX", "Philadelphia, PA",
              "Phoenix, AZ", "San Antonio, TX", "San Diego, CA", "Dallas, TX", "San Jose, CA")
us_rest <- as.data.frame(c())

for (i in 1:length(location)) {
  limit <- 50
  (url <-
      modify_url(yelp, path = c("v3", "businesses", "search"),
                 query = list(term = term, location = location[i], limit = limit)))
  
  res <- GET(url, add_headers('Authorization' = paste("bearer", token)))
  http_status(res)
  
  ct <- content(res)
  tmp <- ct$businesses %>%
    map_df(`[`, c("name", "phone", "is_closed", "rating", "distance", "review_count")) %>%
    mutate(price = ct$businesses[[1]][11], city = ct$businesses[[1]][[12]][4], 
           state = ct$businesses[[1]][[12]][7], zipcode = ct$businesses[[1]][[12]][5], 
           latitude = ct$businesses[[1]][[9]][1], longitude = ct$businesses[[1]][[9]][2]) 
  # `price`, `city`, `state`, `zipcode`, `latitude`, `longitude` were lists within a list 
  # so we need to extract that separately.
  
  us_rest <- rbind(us_rest, tmp)
}

# Convert character lists to just characters and numeric lists to just numerical values
us_rest$price <- as.character(us_rest$price)
us_rest$city <- as.character(us_rest$city)
us_rest$state <- as.character(us_rest$state)
us_rest$zipcode <- as.character(us_rest$zipcode)
us_rest$latitude <- as.numeric(us_rest$latitude)
us_rest$longitude <- as.numeric(us_rest$longitude)

# Change `price` from character to ordinal variable
us_rest <- mutate(us_rest, price_2 = rep(0))

values <- ifelse(us_rest$price == "$", 1, 
       ifelse(us_rest$price == "$$", 2, 
              ifelse(us_rest$price == "$$$", 3, 
                     ifelse(us_rest$price == "$$$$", 4, 5))))

us_rest <- mutate(us_rest, price_2 = values)

# Change `is_closed` from a logical variable to an indicator variable
# where 1 indicated that the restaurant is open and 0 closed
us_rest <- mutate(us_rest, is_open = rep(0))

values <- ifelse(us_rest$is_closed == "FALSE", 1, 0)

us_rest <- mutate(us_rest, is_open = values)
    
# Store test results for each model
test_results <- as.data.frame(c())

```


# Exploratory Analysis
```{r}
# trying to predict the rating
qmplot(longitude, latitude, data = us_rest, colour = I('red'), size = I(3), darken = .3, zoom = 5)
 

us_rest %>%
  filter(rating>=3.5) %>%
    ggplot(aes(x=rating))+geom_histogram(stat="bin", binwidth=.5)

us_rest2 %>%
  filter(!is.na(city))%>%
  group_by(city) %>%
    summarise(avg_rating=mean(rating))%>%
      arrange(avg_rating) %>%
        ggplot(aes(x=city, y=avg_rating))+geom_point()


summary(us_rest2$rating)
```



# Modeling
## Continuous Outcome Variable

### Model 1: Multiple Linear Regression
```{r}
set.seed(1)
# Randomly divide the sample into training and test set
train <- us_rest %>%
  sample_n(450)

test <- us_rest %>%
  anti_join(train, by="phone")

# Fit the model
model_formula <- 
  as.formula("rating ~ review_count + distance + is_open + price_2 + latitude + longitude")
model_mlm <- lm(model_formula, data=train)
# Get predictions from test set
y_hat <- predict(model_mlm, newdata=test)
test <- mutate(test, y_hat = y_hat)


# Determine accuracy of model
test_result <- test %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))
test_result 
test_results <- rbind(test_results, test_result)

# Variable selection with Stepwise Regression
step <- stepAIC(model_mlm, direction="both")
step$anova # display results
```

> Discussion: We decided to start off looking at a model that includes all numeric variables in the training set and compute an initial MSE value from the test set which we can compare with other models. Based on our results we then ran a stepwise regression to determine which model would be the best fit for the data. It turns out that the most accurate linear model only includes `review_count` as a predictor variable, so we will explore the simple linear regression model using `review_count` to predict `rating.`

### Model 2: Simple Linear Regression
```{r}
set.seed(2)
# Randomly divide the sample into training and test set
train <- us_rest %>%
  sample_n(450)

test <- us_rest %>%
  anti_join(train, by="phone")
# Matched by unique phone numbers, but needed to include the one location 
# that does not have a phone number.
#test <- rbind(test, us_rest[1,])

# Fit the model
model_formula <- as.formula("rating ~ review_count")
model_lm <- lm(model_formula, data=train)

# Get predictions from test set
y_hat <- predict(model_lm, newdata=test)
test <- mutate(test, y_hat = y_hat)

# Save fitted model and predictions 
fitted_model <- model_lm %>% 
  broom::augment() %>% 
  as_tibble()
predictions <- model_lm %>% 
  broom::augment(newdata=test)

# Plot 
ggplot(NULL) +
  geom_point(data=fitted_model, aes(x=review_count, y=rating)) +
  geom_line(data=fitted_model, aes(x=review_count, y=.fitted), col="blue") +
  geom_point(data=predictions, aes(x=review_count, y=.fitted), col="red") +
  labs(x="Number of Yelp Reviews", y="Rating", title="Rating by Review Count") + 
  geom_jitter(data=fitted_model, aes(x=review_count, y=rating))

# Residual Visualizations
xyplot(residuals(model_lm) ~ fitted(model_lm), type=c("p","r"), xlab="Predicted values", ylab="Residuals")

histogram(~residuals(model_lm), xlab="Residuals")


# Determine accuracy of model
test_result <- test %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))
test_result 
test_results <- rbind(test_results, test_result)

```

> Discussion: From the scatterplot illustrating the relationship between `rating` and `review_count`, we see that this does not appear to have an apparent strong linear relationship, so perhaps a simple linear regression model based only on review count would not accurately predict the rating of a restaurant.The Mean Squared Error, or MSE, for this model is 0.0807. This appears to be low, but we will need to compare it to the MSE of other models before we can conclude that it is high or low, thus indicating whether or not this is a good model for the prediction of `rating.`
We see that most restaurants have somewhere under 5,000 reviews. 

> Points are evenly scattered above and below the line where residuals equals zero, so this meets the normality condition of linear regression, but does not necessarily mean that the linear model is an appropriate model for this data. 

>(SHOULD WE TRY TO MEET THE CONDITIONS FOR LINEAR REGRESSION OR IS THAT ONLY FOR EXPLANATORY REGRESSION RATHER THIS PREDICTION REGRESSION????)



### Model 3: Spline
```{r}
set.seed(3)
# Randomly divide the sample into training and test set
train <- us_rest %>%
  sample_n(450)

test <- us_rest %>%
  anti_join(train, by="phone")

# Fit the model
model_spline <- smooth.spline(x=train$review_count, y=train$rating, df=10) 
# play around with degrees of freedom

# Get predictions from test set
y_hat <- predict(model_spline, x=test$review_count)
test <- mutate(test, y_hat = y_hat$y)


# Determine accuracy of model
MSE <- mean((test$rating - test$y_hat)^2)

test_result <- test %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))
test_result 
test_results <- rbind(test_results, test_result)
```

> Discussion: Explain what is a spline model. Used df=10 because... Does not have a lower MSE value compared to the simple linear regression model so we move on to LOESS


### Model 4: LOESS
```{r}
set.seed(5)
# Randomly divide the sample into training and test set
train <- us_rest %>%
  sample_n(450)

test <- us_rest %>%
  anti_join(train, by="phone")

# Fit the model
model_formula <- as.formula("rating ~ review_count")
model_loess <- loess(model_formula, data=train, span=0.9)

# Get predictions from test data
y_hat <- predict(model_loess, newdata=test)
test <- mutate(test, y_hat = y_hat)

# Plot
fitted_model <- model_loess %>% 
  broom::augment() %>% 
  as_tibble()
predictions <- model_loess %>% 
  broom::augment(newdata=test) %>% 
  as_tibble()

ggplot(NULL) +
  geom_point(data=fitted_model, aes(x=review_count, y=rating)) +
  geom_line(data=fitted_model, aes(x=review_count, y=.fitted), col="blue") +
  geom_point(data=predictions, aes(x=review_count, y=.fitted), col="red") +
  labs(x="Review Count", y="Rating", title="Rating by Review Count (LOESS)")

# Determine accuracy of model
test_result <- test %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))
test_result # Why are we getting NA's here
test_results <- rbind(test_results, test_result)
```

> Discussion: 

## Categorical Outcome Variable with > 2 levels
### CART
```{r}
set.seed(5)
# Randomly divide the sample into training and test set
train <- us_rest %>%
  sample_n(450)

test <- us_rest %>%
  anti_join(train, by="phone")

# Fit the model
model_formula <- as.formula("rating ~ review_count")
tree_parameters <- rpart.control(maxdepth = 3)
model_CART <- rpart(model_formula, data = train, control=tree_parameters)


# Tree Diagram
plot(model_CART, margin=0.25)
text(model_CART, use.n = TRUE)
title("Predicting Rating using Review Count")
box()


# predictions are not working
p_hat_matrix <- model_CART %>% 
  predict(type = "prob", newdata = test)

y_hat <- model_CART %>% 
  predict(newdata=test, type="class")

# Score/error
MLmetrics::Accuracy(y_true = iris$Species, y_pred = y_hat)
MLmetrics::ConfusionMatrix(y_true = iris$Species, y_pred = y_hat)
```



# Extension
> test on a smaller city, i.e. not one of the United States largest cities for an out-of-sample prediction, and see if the best model still does well on a different area. If not, speculate as to why this might be the case.


# Questions
> Could we treat `rating` as a categorical variable with multiple levels
> Could you use a 2 sample t-test to determine accuracy of predictions or does this lead to overfitting?

