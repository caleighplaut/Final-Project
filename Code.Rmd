---
title: "Code"
author: "Caleigh Plaut and Meredith Manley"
date: "12/18/17"
output: 
  html_document:
    fig_height: 3
    fig_width: 5
  pdf_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---

## Load the Data
```{r}
# Load the necessary packages
require(mosaic)
library(tidyverse)
library(httr)
library(yelpr)
library(glmnet)
library(MASS)
library(ggplot2)
library(rpart)
library(ggmap)

us_rest <- read_csv("us_rest.csv")
    
# Store test results for each model
test_results <- as.data.frame(c())

```


## Exploratory Analysis
#### Figure 1: Map of 10 Largest Cities in the US
```{r}
qmplot(longitude, latitude, data = us_rest, colour = I('red'), size = I(3), darken = .3, zoom = 5)
``` 

> 



#### Figure 2: Distribution of Outcome Variable  - `rating`
```{r}
us_rest %>%
  filter(rating>=3.5) %>%
    ggplot(aes(x=rating))+geom_histogram(stat="bin", binwidth=.5) + 
    labs(title = "Count of US Restaurants by Respective Rating")
```

> As you can see, most resutrants got a rating of 4.0, which is high since the ratings are 1-5. All restaurant ratings are somewhat bias upwards, so finding one under 4.0 is fairly rare. Also, finding a resutrant with a rating of 5.0 is rare.  Shows the distribution of our outcome variable, which given the limited range of ratings, appears to be fairly normal. Looking at this plot is important because had it been significantly skewed to the right or to the left we may have needed to include some kind of transformation before we began developing our models.


#### Figure 3: Association Between `distance` and `rating`
```{r}
us_rest %>%
        ggplot(aes(x=distance, y=rating)) + geom_point() +
        labs(title = "Association Between Distance and Rating")
```

> The distance variable refers to how far away a given restaurant is from the city that was searched measured in meters regardless of what metric system the desired city uses. One hypothesis that we had was that restaurants that are further from the city center would have a lower rating because they do not experience as much traffic as those downtown and thus may not have as many reviews or customers to rate their experience. From this plot we can see that is it hard to determine if that is true or not.  However, we can see that those restaurants that received the highest rating of 5.0 were all  within approximately 8,000 meters or 5 miles from the searched city and those cities that were furthest away from the searched city received lower ratings of 4.0. This observations are not conclusive in terms of predictability especially given that the correlation between distance and rating is close to zero (-0.007929708), but provide some evidence that we should include this variable in our model development.


#### Figure 4: Association Between `review_count` and `rating` Relative to `price`
```{r}
us_rest %>%
  filter(!is.na(city))%>%
        ggplot(aes(x=review_count, y=rating, color = price_2)) + geom_point(position = "jitter", alpha = 0.4) + labs(title = "Assocation between Review Count and Rating", x = "review count") #+ geom_smooth() 
cor(us_rest$review_count, us_rest$rating)
```

> In this plot we see review count on the x axis and restaurant rating on the y-axis with color indicating the price. This scatterplot differs from the previous plot of the relationship between rating and distance because we have used the jitter feature within the ggplot package so that we can more easily see the different price value for each unique observation. However it does not appear that price varies however remains constant across the plot, so from this graphic it does not appear that price would have a predictive effect on rating. On the other hand, review_count has a moderately strong negative correlation of -0.2554035 with rating and thus we would cautiously say that as review count increases the rating of a given establishment seems to decrease. Given this analysis it would also be beneficial to include this variable in the model development.

# Results


# Diagnostics
### Continuous Outcome Variable

#### Model 1: Multiple Linear Regression
```{r}
set.seed(1)
# Randomly divide the sample into training and test set
train <- us_rest %>%
  sample_n(450)

test <- us_rest %>%
  anti_join(train, by="phone")
# Matched by unique phone numbers, but needed to include the one location 
# that does not have a phone number.
test <- rbind(test, us_rest[1,])

# Fit the model
model_formula <- 
  as.formula("rating ~ review_count + distance + is_open + price_2 + latitude + longitude")
model_mlm <- lm(model_formula, data=train)

# calculate train MSE for comparison
y_hat <- predict(model_mlm, newdata=train)
train <- mutate(train, y_hat = y_hat)


# Determine accuracy of model
check <- train %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))


# Get predictions from test set
y_hat <- predict(model_mlm, newdata=test)
test <- mutate(test, y_hat = y_hat)


# Determine accuracy of model
test_result <- test %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))
test_result 
test_results <- rbind(test_results, test_result)

# Variable selection with Stepwise Regression
step <- stepAIC(model_mlm, direction="both")
step$anova # display results
```

> Discussion: We decided to start off looking at a model that includes all numeric variables in the training set and compute an initial MSE value from the test set which we can compare with other models. Based on our results we then ran a stepwise regression to determine which model would be the best fit for the data. It turns out that the most accurate linear model only includes `review_count` as a predictor variable, so we will explore the simple linear regression model using `review_count` to predict `rating.` 


#### Model 2: Spline
```{r}
set.seed(1)
# Randomly divide the sample into training and test set
train <- us_rest %>%
  sample_n(450)

test <- us_rest %>%
  anti_join(train, by="phone")
# Matched by unique phone numbers, but needed to include the one location 
# that does not have a phone number.
test <- rbind(test, us_rest[1,])

# Fit the model
model_spline <- smooth.spline(x=train$review_count, y=train$rating, df=10) 
# play around with degrees of freedom

# Calculate train MSE for comparison
y_hat <- predict(model_spline, x=train$review_count)
train <- mutate(train, y_hat = y_hat$y)

check <- train %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))
check



# Get predictions from test set
y_hat_test <- predict(model_spline, x=test$review_count)
test <- mutate(test, y_hat = y_hat_test$y)

# plot
fitted_model <- model_spline %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  rename(review_count = x, rating = y)
predictions <- test %>% 
  mutate(.fitted = y_hat)

ggplot(NULL) +
  geom_point(data=fitted_model, aes(x=review_count, y=rating), position = "jitter") +
  geom_line(data=fitted_model, aes(x=review_count, y=.fitted), col="blue") +
  geom_point(data=predictions, aes(x=review_count, y=.fitted), col="red") +
  labs(x="Count of Reviews", y="Rating", title = "Spline Model predicting Rating")

# Determine accuracy of model
# used when determining the appropriate degrees of freedom for this model
MSE <- mean((test$rating - test$y_hat)^2)

test_result <- test %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))
test_result 
test_results <- rbind(test_results, test_result)
```

> Discussion: Explain what is a spline model. Used df=10 because... Does not have a lower MSE value compared to the simple linear regression model


### Categorical Outcome Variable with > 2 levels
#### Model 3: CART
```{r}
set.seed(1)
# Randomly divide the sample into training and test set
train <- us_rest %>%
  sample_n(450)

test <- us_rest %>%
  anti_join(train, by="phone")
# Matched by unique phone numbers, but needed to include the one location 
# that does not have a phone number.
test <- rbind(test, us_rest[1,])
test$rating <- as.character(test$rating)

# because the test set does not possess any `rating` observations that are below
# 4.0 and there are only 6 observations that are below 4.0 in the training set then
# we will get ride of these few observations and proceed for the classification
# algorithm
train <- train[-(c(56, 108, 181, 220, 270, 380)), ]
train$rating <- as.character(train$rating)


# Fit the model
model_formula <- 
  as.formula("rating ~ review_count + distance + is_open + price_2 + latitude + longitude + city")
tree_parameters <- rpart.control(maxdepth = 3)
model_CART <- rpart(model_formula, data = train, control=tree_parameters)

# compare the score of the training set with the test set
p_hat_matrix <- model_CART %>% 
  predict(type = "prob", newdata = train)

y_hat <- model_CART %>% 
  predict(newdata=train, type="class")

# Score/error
MLmetrics::Accuracy(y_true = train$rating, y_pred = y_hat) 
MLmetrics::ConfusionMatrix(y_true = train$rating, y_pred = y_hat)


# predictions on test set
p_hat_matrix <- model_CART %>% 
  predict(type = "prob", newdata = test)

y_hat <- model_CART %>% 
  predict(newdata=test, type="class") %>%
  as.character() 
test <- mutate(test, y_hat = y_hat)

# Percentage of correct ratings
MLmetrics::Accuracy(y_true = test$rating, y_pred = y_hat) 