---
title: "Code"
author: "Caleigh Plaut and Meredith Manley"
date: "12/18/17"
output: 
  html_document:
    fig_height: 3
    fig_width: 5
  pdf_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---

## Load the Data
```{r}
# Load the necessary packages
require(mosaic)
library(tidyverse)
library(httr)
library(yelpr)
library(glmnet)
library(MASS)
library(ggplot2)
library(rpart)
library(ggmap)

us_rest <- read_csv("us_rest.csv")
```


# Exploratory Analysis
#### Figure 1: Map of 10 Largest Cities in the US
```{r}
qmplot(longitude, latitude, data = us_rest, colour = I('red'), size = I(3), darken = .3, zoom = 5)
``` 


#### Figure 2: Distribution of Outcome Variable  - `rating`
```{r}
us_rest %>%
  filter(rating>=3.5) %>%
    ggplot(aes(x=rating))+geom_histogram(stat="bin", binwidth=.5) + 
    labs(title = "Count of US Restaurants by Respective Rating")
```


#### Figure 3: Association Between `distance` and `rating`
```{r}
us_rest %>%
        ggplot(aes(x=distance, y=rating)) + geom_point() +
        labs(title = "Association Between Distance and Rating")
```


#### Figure 4: Association Between `review_count` and `rating` Relative to `price`
```{r}
us_rest %>%
  filter(!is.na(city))%>%
        ggplot(aes(x=review_count, y=rating, color = price_2)) + geom_point(position = "jitter", alpha = 0.4) + labs(title = "Assocation between Review Count and Rating", x = "review count") #+ geom_smooth() 
cor(us_rest$review_count, us_rest$rating)
```

# Modeling
### Continuous Outcome Variable

#### Model 1: Multiple Linear Regression
```{r}
set.seed(1)
# Randomly divide the sample into training and test set
train <- us_rest %>%
  sample_n(450)

test <- us_rest %>%
  anti_join(train, by="phone")
# Matched by unique phone numbers, but needed to include the one location 
# that does not have a phone number.
test <- rbind(test, us_rest[1,])

# Fit the model
model_formula <- 
  as.formula("rating ~ review_count + distance + is_open + price_2 + latitude + longitude")
model_mlm <- lm(model_formula, data=train)

# calculate train MSE for comparison
y_hat <- predict(model_mlm, newdata=train)
train <- mutate(train, y_hat = y_hat)


# Determine accuracy of model
check <- train %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))


# Get predictions from test set
y_hat <- predict(model_mlm, newdata=test)
test <- mutate(test, y_hat = y_hat)


# Determine accuracy of model
test_result <- test %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))

# Variable selection with Stepwise Regression
step <- stepAIC(model_mlm, direction="both")
step$anova # display results
```


#### Model 2: Spline
```{r}
set.seed(1)
# Randomly divide the sample into training and test set
train <- us_rest %>%
  sample_n(450)

test <- us_rest %>%
  anti_join(train, by="phone")
# Matched by unique phone numbers, but needed to include the one location 
# that does not have a phone number.
test <- rbind(test, us_rest[1,])

# Fit the model
model_spline <- smooth.spline(x=train$review_count, y=train$rating, df=10) 
# play around with degrees of freedom

# Calculate train MSE for comparison
y_hat <- predict(model_spline, x=train$review_count)
train <- mutate(train, y_hat = y_hat$y)

check <- train %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))



# Get predictions from test set
y_hat_test <- predict(model_spline, x=test$review_count)
test <- mutate(test, y_hat = y_hat_test$y)

# plot
fitted_model <- model_spline %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  rename(review_count = x, rating = y)
predictions <- test %>% 
  mutate(.fitted = y_hat)

ggplot(NULL) +
  geom_point(data=fitted_model, aes(x=review_count, y=rating), position = "jitter") +
  geom_line(data=fitted_model, aes(x=review_count, y=.fitted), col="blue") +
  geom_point(data=predictions, aes(x=review_count, y=.fitted), col="red") +
  labs(x="Count of Reviews", y="Rating", title = "Spline Model predicting Rating")

# Determine accuracy of model
# used when determining the appropriate degrees of freedom for this model
MSE <- mean((test$rating - test$y_hat)^2)

test_result <- test %>%
  summarise(MAE = sum(abs(y_hat - rating))/n(),
            MSE = sum((y_hat - rating)^2)/n(),
            SSE = sum((y_hat - rating)^2))
```


### Categorical Outcome Variable with > 2 levels
#### Model 3: CART
```{r}
set.seed(1)
# Randomly divide the sample into training and test set
train <- us_rest %>%
  sample_n(450)

test <- us_rest %>%
  anti_join(train, by="phone")
# Matched by unique phone numbers, but needed to include the one location 
# that does not have a phone number.
test <- rbind(test, us_rest[1,])
test$rating <- as.character(test$rating)

# because the test set does not possess any `rating` observations that are below
# 4.0 and there are only 6 observations that are below 4.0 in the training set then
# we will get ride of these few observations and proceed for the classification
# algorithm
train <- train[-(c(56, 108, 181, 220, 270, 380)), ]
train$rating <- as.character(train$rating)


# Fit the model
model_formula <- 
  as.formula("rating ~ review_count + distance + is_open + price_2 + latitude + longitude + city")
tree_parameters <- rpart.control(maxdepth = 3)
model_CART <- rpart(model_formula, data = train, control=tree_parameters)

# compare the score of the training set with the test set
p_hat_matrix <- model_CART %>% 
  predict(type = "prob", newdata = train)

y_hat <- model_CART %>% 
  predict(newdata=train, type="class")

# Score/error
MLmetrics::Accuracy(y_true = train$rating, y_pred = y_hat) 
MLmetrics::ConfusionMatrix(y_true = train$rating, y_pred = y_hat)


# predictions on test set
p_hat_matrix <- model_CART %>% 
  predict(type = "prob", newdata = test)

y_hat <- model_CART %>% 
  predict(newdata=test, type="class") %>%
  as.character() 
test <- mutate(test, y_hat = y_hat)

# Percentage of correct ratings
MLmetrics::Accuracy(y_true = test$rating, y_pred = y_hat) 
```

